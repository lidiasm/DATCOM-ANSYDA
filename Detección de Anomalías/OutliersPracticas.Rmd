---
title: "Guión de prácticas para la Detección de Anomalías"
author: "Lidia Sánchez Mérida"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reshape)
library(fitdistrplus)
library(outliers)
library(MVN)
library(CerioliOutlierDetection)
library(mvoutlier)
library(ggbiplot)
library(DDoutlier)
library(cluster)

# Importamos todas las funciones del script `OutliersFunciones_byCubero.R`
setwd("~/Escritorio/ANSDA/Prácticas Anomalías")
source("OutliersFunciones_byCubero.R")
```

## 2. Dataset y selección de variables
Cargamos el dataset `mtcars` disponible en R directamente y lo filtramos por las columnas numéricas. Luego eliminamos aquellas con poca variedad de valores y también si tienen valores perdidos.

```{r}
# Cargamos el conjunto de datos
datos = mtcars
head(datos)
# Obtenemos las columnas numéricas
columnas.num = sapply(c(1:ncol(datos)) , function(x) is.numeric(datos[, x]))
# Construimos un nuevo dataset 
datos.num = datos[, columnas.num]
head(datos.num)
# Eliminamos las variables con poca variabilidad de datos
datos.num  = datos.num[,-c(2 , 8:11)]  
head(datos.num)
# Eliminamos registros con valores NAN
datos.num = na.omit(datos.num)
```

## 3. Detección de outliers en una dimensión
### 3.1. Outliers IQR

Parece que la distribución de la mayoría de variables es similar a la distribución normal.

```{r}
# Grid 2x3
par(mfrow = c(2,3))
# Histogramas de las variables
columnas.num = sapply(c(1:ncol(datos)) , function(x) hist(datos[, x], main="", xlab=names(datos)[x]))
# Guardamos ciertas variables para usos posteriores
indice.columna = 1
columna = datos.num[, indice.columna]
nombre.columna = names(datos.num) [indice.columna]
```

#### 3.1.1. Obtención de los outliers IQR

Calculamos los Q1 y Q3 para luego obtener los intervalos que detectan tanto `outliers moderados como outliers extremos`.

```{r}
indice.columna
nombre.columna
# Q1 
cuartil.primero = quantile(datos.num[, indice.columna], prob=seq(0.25, 0.25, 0))
cuartil.primero
# Q3 
cuartil.tercero = quantile(datos.num[, indice.columna], prob=seq(0.75, 0.75, 0))
cuartil.tercero
# IQR 
iqr = IQR(datos.num[, indice.columna])
iqr
# Intervalo de outliers moderados
extremo.superior.outlier.IQR = cuartil.tercero + 1.5*iqr
extremo.superior.outlier.IQR
extremo.inferior.outlier.IQR = cuartil.primero - 1.5*iqr
extremo.inferior.outlier.IQR
# Intervalo de outliers extremos
extremo.superior.outlier.IQR.extremo = cuartil.tercero + 3*iqr
extremo.superior.outlier.IQR.extremo
extremo.inferior.outlier.IQR.extremo = cuartil.primero - 3*iqr
extremo.inferior.outlier.IQR.extremo
# Buscar outliers moderados
son.outliers.IQ = datos.num[, indice.columna] < extremo.inferior.outlier.IQR | datos.num[, indice.columna] > extremo.superior.outlier.IQR
head(son.outliers.IQ)
sum(son.outliers.IQ)
# Buscar outliers extremos
son.outliers.IQ.extremos = datos.num[, indice.columna] < extremo.inferior.outlier.IQR.extremo | datos.num[, indice.columna] > extremo.superior.outlier.IQR.extremo
head(son.outliers.IQ.extremos)
sum(son.outliers.IQ.extremos)
```
#### 3.1.2. Índices y valores de los outliers IQR

Se obtienen los datasets que contienen tanto los `outliers moderados como los outliers extremos`.

```{r}
### OUTLIERS MODERADOS ###
# Obtenemos los índices de las muestras con outliers. Solo hay 1 y tiene como índice el 20
claves.outliers.IQR = which(son.outliers.IQ==TRUE)
claves.outliers.IQR
# Obtenemos el dataset con las muestras con outliers. Para ello utilizamos la variable anteriormente
# creada que determina si cada registro es outlier o no
df.outliers.IQR = datos %>% filter(son.outliers.IQ)
df.outliers.IQR
# Obtenemos los nombres de la fila
nombres.outliers.IQR = rownames(df.outliers.IQR)
nombres.outliers.IQR
# Obtenemos los valores de la fila como vector
valores.outliers.IQR = df.outliers.IQR[,1]
valores.outliers.IQR

### OUTLIERS EXTREMOS ###
# Obtenemos los índices de las muestras con outliers. Solo hay 1 y tiene como índice el 20
claves.outliers.IQR.extremos = which(son.outliers.IQ.extremos==TRUE)
claves.outliers.IQR.extremos
# Obtenemos el dataset con las muestras con outliers. Para ello utilizamos la variable anteriormente
# creada que determina si cada registro es outlier o no
df.outliers.IQR.extremos = datos %>% filter(son.outliers.IQ.extremos)
df.outliers.IQR.extremos
# Obtenemos los nombres de la fila
nombres.outliers.IQR.extremos = rownames(df.outliers.IQR.extremos)
nombres.outliers.IQR.extremos
# Obtenemos los valores de la fila como vector
valores.outliers.IQR.extremos = df.outliers.IQR.extremos[,1]
valores.outliers.IQR.extremos
```
#### 3.1.3 Cómputo de los outliers IQR con funciones

En esta sección se obtienen los mismos valores que en el apartado anterior pero con algunas funciones definidas.

```{r}
# Obtenemos todos los datos anteriores pero con las funciones definidas en este chunk
son.outliers.IQR = son_outliers_IQR(datos.num, indice.columna)
head(son.outliers.IQR)
claves.outliers.IQR  = claves_outliers_IQR (datos.num, indice.columna)
claves.outliers.IQR
son.outliers.IQR.extremos = son_outliers_IQR (datos.num, indice.columna, 3)
head(son.outliers.IQR.extremos)
claves.outliers.IQR.extremos = claves_outliers_IQR (datos.num, indice.columna, 3)
claves.outliers.IQR.extremos
```
#### 3.1.4 Desviación de los outliers con respecto a la media de la columna

En este apartado se pretende considerar si un valor es outlier en dos situaciones diferentes:

1. La situación estándar en la que se consideran intervalos [-2, 2] para outliers moderados y [-3, 3] para outliers extremos. Estos intervalos se suelen utilizar para **variables que sigan tanto una normal como una similar.**

2. Una segunda situación que se basa en los intervalos calculados a partir del IQR. Estos valores **solo se utilizan para variables con distribuciones normales o variables con distribuciones similares, o que al menos no rechacen los tests de normalidad.**

```{r}
# Escalamos el dataset de variables numéricas
datos.num.norm = scale(datos.num)
head(datos.num.norm)
# Obtenemos los datos escalados de la columna `mpg`
columna.norm = datos.num.norm[, indice.columna]
columna.norm
# Obtenemos los valores normalizados de los outliers
valores.outliers.IQR.norm = columna.norm[claves.outliers.IQR]
valores.outliers.IQR.norm
# Obtenemos todos los datos normalizados del outlier encontrado
datos.num.norm.outliers.IQR = datos.num.norm[claves.outliers.IQR,]
datos.num.norm.outliers.IQR
```

#### 3.1.5 Gráfico

En esta sección se grafican los datos normalizados los **outliers moderados y extremos**.

```{r}
# Outliers moderados y extremos 
plot_2_colores(datos.num$mpg, claves.outliers.IQR)
plot_2_colores(datos.num$mpg, claves.outliers.IQR.extremos)
```
#### 3.1.6 Diagramas de cajas

En principio mostramos los diagramas de cajas de la variable `mpg` con los datos sin normalizar y los datos normalizados puesto que este proceso no afecta a su posición. En rojo aparece el `outlier moderado` que hemos identificado anteriormente.

```{r}
# Representamos los diagramas de cajas para la variable `mpg` sin normalizar y normalizada
diag_caja_outliers_IQR(datos.num, indice.columna)
diag_caja_outliers_IQR(datos.num.norm, indice.columna)
```

Ahora realizamos la misma representación anterior pero mostrando el nombre del `outlier moderado`.

```{r}
# Realizamos la misma representación anterior pero dibujando el outlier
diag_caja(datos.num, indice.columna, claves.outliers.IQR)
```
A continuación representamos los `outliers` encontrados en cada una de las variables del dataset. Se demuestra que **mientras una muestra puede ser un `outlier` para una columna, para el resto puede que no lo sea.**

```{r}
diag_caja_juntos(datos, claves.a.mostrar=claves.outliers.IQR)
```
### 3.2. Tests de hipótesis

#### 3.2.1. Objetivo

El objetivo en este apartado consiste en confirmar o rechazar que el dato encontrado anteriormente es verdaderamente un `outlier`. En primer lugar comprobamos gráficamente que la columna en la que lo hemos encontrado sigue una distribución normal o similar.

```{r}
ajusteNormal = fitdist(columna, "norm")
denscomp (ajusteNormal,  xlab = nombre.columna)
```
#### 3.2.3 Test de Grubbs

A continuación planteamos la hipótesis nula de que no es un outlier y la hipótesis alternativa que sí que lo es. Como el `p-value=0.55` y por tanto no es menor que `alpha=0.05` no se rechaza la hipótesis nula por lo que **no se puede confirmar que sea un ´outlier`**. De este modo, solo nos queda comprobar **cuál es el valor que más se aleja de la media**. Para ello lo vemos con la función `outlier()`.

```{r}
test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value
# Valor más lejano de la media
valor.posible.outlier = outlier(columna)
valor.posible.outlier
es.posible.outlier = outlier(columna, logical = TRUE)
es.posible.outlier
clave.posible.outlier = which( es.posible.outlier == TRUE)
clave.posible.outlier
```
#### 3.2.4 Test de Normalidad

Ahora trabajaremos con otro dataset para intentar ver la distribución que siguen las variables **tras eliminar los outliers**. Para ello planteamos una H0=la distribución es normal y H1=la distribución no es normal.

```{r}
# Nuevo dataset
datos.artificiales = c(45,56,54,34,32,45,67,45,67,65,140)
# Detectamos el outlier
test.de.Grubbs = grubbs.test(datos.artificiales, two.sided = TRUE)
test.de.Grubbs$p.value
# Obtenemos el dato outlier
valor.posible.outlier = outlier(datos.artificiales)
valor.posible.outlier
es.posible.outlier = outlier(datos.artificiales, logical = TRUE)
es.posible.outlier
clave.posible.outlier = which(es.posible.outlier == TRUE)
clave.posible.outlier
```
Eliminamos el anterior `outlier` detectado para comprobar la normalidad de los datos. Como el `p-value=0.7` **no se puede rechazar la H0 por lo que se puede asumir que los datos siguen una distribución normal**.

```{r}
# Eliminamos el outlier del dataset
datos.artificiales.sin.outlier = datos.artificiales[-clave.posible.outlier]
datos.artificiales.sin.outlier
# Test de normalidad
shapiro.test(datos.artificiales.sin.outlier)
goodness_fit = gofstat(ajusteNormal)
goodness_fit$adtest
```

A continuación se implementa una función que aplique todo el procedimiento anterior siguiendo estos pasos:

1. Aplicamos el test de Grubbs para ver si hay `outliers` en la columna proporcionada.
2. A continuación obtenemos los `outliers` encontrados así como sus posiciones dentro del dataset.
3. Comapramos su distribución con la normal. Si lo es, podremos aplicar el test de `Shapiro` para comprobar si la variable sigue una distribución normal tras eliminar los `outliers` encontrados.

```{r}
# Aplica el test de Grubbs sobre la columna ind.col de datos y devuelve una lista con:
# nombre.columna: Nombre de la columna datos[, ind.col]
# clave.mas.alejado.media: Clave del valor O que está más alejado de la media
# valor.mas.alejado.media: Valor de O en datos[, ind.col]
# nombre.mas.alejado.media: Nombre de O en datos
# es.outlier: TRUE/FALSE dependiendo del resultado del test de Grubbs sobre O
# p.value:  p-value calculado por el test de Grubbs
# es.distrib.norm: Resultado de aplicar el test de Normalidad 
#    de Shapiro-Wilks sobre datos[, ind.col]
#    El test de normalidad se aplica sin tener en cuenta el 
#    valor más alejado de la media (el posible outlier O)
#    TRUE si el test no ha podido rechazar
#       -> Sólo podemos concluir que los datos no contradicen una Normal
#    FALSE si el test rechaza 
#       -> Los datos no siguen una Normal
test_Grubbs = function(data.frame, indice.columna, alpha = 0.05) {
  # Aplicamos el test de Grubbs
  test.de.Grubbs = grubbs.test(data.frame[, indice.columna], two.sided = TRUE)
  # Buscamos las posiciones de los valores más alejados de la media
  posibles.outliers = outlier(data.frame[, indice.columna], logical = TRUE)
  clave.mas.alejado.media = which(posibles.outliers == TRUE)
  valor.mas.alejado.media = data.frame[clave.mas.alejado.media, indice.columna]
  # Test de normalidad sin los outliers
  data.frame.sin.outlier = data.frame[-clave.mas.alejado.media, indice.columna]
  shapiro.resultados = shapiro.test(data.frame.sin.outlier)

  # Valores a devolver
  result = data.frame(
    nombre.columna=names(data.frame)[indice.columna],
    clave.mas.alejado.media=clave.mas.alejado.media,
    valor.mas.alejado.media=valor.mas.alejado.media,
    es.outlier=test.de.Grubbs$p.value < 0.05,
    p.value=test.de.Grubbs$p.value,
    p.value.test.normalidad=shapiro.resultados$p.value,
    es.distrib.norm=shapiro.resultados$p.value > 0.05
  )
  return (result)
}

# Aplicamos la función anterior
df.datos.artificiales = as.data.frame(datos.artificiales)
test.Grubbs.datos.artificiales = test_Grubbs(df.datos.artificiales, 1)
test.Grubbs.datos.artificiales
```
### 3.3 Trabajando con varias columnas

#### 3.3.1 Outliers IQR

En este apartado se pretenden calcular los `outliers` a partir de **IQR** para detectarlos con respecto a cada una de las variables de un dataset.

```{r}
# Obtenemos los registros outliers con respecto a alguna columna
claves.outliers.IQR.en.alguna.columna = claves_outliers_IQR_en_alguna_columna(datos.num, 1.5)
claves.outliers.IQR.en.alguna.columna
# Eliminamos registros repetidos por haber sido outliers en varias columnas
claves.outliers.IQR.en.mas.de.una.columna = 
  unique(
    claves.outliers.IQR.en.alguna.columna[
      duplicated(claves.outliers.IQR.en.alguna.columna)])
claves.outliers.IQR.en.mas.de.una.columna
claves.outliers.IQR.en.alguna.columna = 
  unique (claves.outliers.IQR.en.alguna.columna)
claves.outliers.IQR.en.alguna.columna 
nombres_filas(datos.num, claves.outliers.IQR.en.mas.de.una.columna)
# Mostramos los nombres de los coches outliers para alguna de las variables
nombres_filas(datos.num, claves.outliers.IQR.en.alguna.columna)
# Valores normalizados de estos outliers
outliers.norm.frame = datos.num.norm[claves.outliers.IQR.en.alguna.columna, ]
# Representamos los diagramas de cajas de todas las variables con los outliers
diag_caja_juntos(datos.num.norm, claves.a.mostrar=claves.outliers.IQR.en.alguna.columna)
```

#### 3.3.2 Tests de Hipótesis (OPCIONAL)

En este apartado el objetivo consiste en aplicar la función `test_Grubbs` definida anteriormente para aplicar el **test de Grubbs sobre todas las variables** y comprobar si siguen o no una distribución normal o similar. Como podemos observar en los resultados, las **variables `disp` y `drat` han dado negativo en el test por lo que se rechaza la hipótesis nula de ser variables normales.

```{r}
# Representamos las distribuciones de los datos normalizados
par(mfrow = c(3,3))
sapply(c(1:ncol(datos.num)) , function(x) denscomp (fitdist(datos.num[, x], "norm"), main="", xlab=x))
# Aplicamos el test de Grubbs a todas las variables
sapply(c(1:ncol(datos.num)) , function(x) test_Grubbs(datos.num, x))
```
## 4. Outliers Multivariantes

### 4.1. Métodos estadísticos basados en la distancia de Mahalanobis

El objetivo de este cuarto apartado es encontrar los valores *outliers* utilizando métodos estadísticos que nos garanticen su naturaleza. Por lo tanto, la **hipótesis nula es que no es un outlier** para que si rechaza confirmemos que lo es.

#### 4.1.1 Hipótesis de Normalidad

Para aplicar los métodos basados en la **distancia de Mahalanobis** todas las variables deben seguir una distribución normal univariante y multivariante en conjunto. Por ello se debe aplicar un test de normalidad multivariante. Al tener un p-valor menor que el umbral=0.05 se rechaza la hipótesis nula y por tanto **no siguen una distribución normal multivariante** por lo que no se debería aplicar dicho método.

```{r}
# Obtenemos las columnas que siguen una distribución normal
resultados = sapply(c(1:ncol(datos.num)), function(x)test_Grubbs(datos.num, x))
son.col.normales = unlist(resultados[7, ]) # unlist() = list->vector
son.col.normales
# Construimos un dataset solo con las columnas que siguen una distribución normal
datos.num.distrib.norm = datos.num[,unlist(son.col.normales)] 
datos.num.distrib.norm
# Test de normalidad multivariante
test.MVN = mvn(datos.num.distrib.norm, mvnTest = "energy")
test.MVN$multivariateNormality["MVN"]
test.MVN$multivariateNormality["p value"]
```

#### 4.1.2 Tests de hipótesis para detectar outliers

```{r}
# Comparación de la detección de outliers utilizando una técnica clásica y otra más robusta
mvoutlier::corr.plot(datos.num[,1], datos.num[,2])
```
En primer lugar aplicamos un **test individual** para calcular los outliers con mayor distancia de Mahalanobis al resto de los datos. Como resultado obtenemos cuatro valores outliers. Mientras que con un **test múltiple** con una penalización concreta probamos si cada valor es un outlier o no. En este conjunto de test no se obtiene ningún outlier.

```{r}
set.seed(2)
# Aplicamos un test individual con 0.05
test.individual = cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha=0.05)
claves.test.individual = unlist(which(test.individual$outliers))
claves.test.individual
nombres.test.individual = nombres_filas(datos.num.distrib.norm, claves.test.individual)
nombres.test.individual
# Aplicamos múltiples tests con una corrección de Sidak
test.interseccion = cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha=1-(((1-0.05)^(1/dim(datos.num.distrib.norm)[1]))))
claves.test.interseccion = unlist(which(test.interseccion$outliers))
claves.test.interseccion
nombres.test.interseccion = nombres_filas(datos.num.distrib.norm, claves.test.interseccion)
nombres.test.interseccion
# Ordenamos decrecientemente el vector de outliers para ver cuál es el mayor
test.individual$mahdist
clave.mayor.dist.Mah = order(test.individual$mahdist.rw, decreasing=TRUE)[1]
clave.mayor.dist.Mah
nombre.mayor.dist.Mah = test.individual$mahdist[clave.mayor.dist.Mah]
nombre.mayor.dist.Mah
# Mostramos las distancias de Mahalanobis
plot(sort(test.individual$mahdist))
```
### 4.2 Visualización de datos con un Biplot

Los gráficos **BiPlot** permiten hacernos una idea aproximada de los valores de las muestras con respecto a todas las variables además de las correlaciones entre sí.

```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num, 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```
### 4.3 Métodos basados en distancias: LOF

Los métodos basados en distancia se utilizan cuando se desea buscar `outliers` y no se cumplen las condiciones de normalidad. Por defecto, se utiliza la distancia Euclídea. Para ello se deberán normalizar los datos previamente puesto que estos métodos son sensibles a las escalas de las variables.

```{r}
# Aplicamos LOF con k=5
num.vecinos.lof = 5
lof.scores = LOF(datos.num.norm, k = num.vecinos.lof)
# Ordenamos los scores decrecientemente y mostragmos un gráfico
sorted.lof.scores = sort(lof.scores, decreasing=TRUE)
plot(sorted.lof.scores)
# Obtenemos los tres valores más extremos que aparecen en el gráfico
num.outliers = 3
claves.outliers.lof = sapply(c(1:num.outliers), function(x) which(lof.scores==sorted.lof.scores[x]))
claves.outliers.lof
nombres.outliers.lof = nombres_filas(datos.num.norm, claves.outliers.lof)
nombres.outliers.lof
# Mostramos los valores normalizados de los registros
datos.num.norm[claves.outliers.lof, ]
```
Vamos a analizar las interacciones existentes entre cada par de variables destacando en rojo el outlier con mayor valor hasta el momento.

```{r}
clave.max.outlier.lof = claves.outliers.lof[1]
colores = rep("black", times = nrow(datos.num.norm))
colores[clave.max.outlier.lof] = "red"
pairs(datos.num.norm, pch = 19,  cex = 0.5, col = colores, lower.panel = NULL)
```
Según parece existe una correlación entre las dos primeras variables en las que el outlier se sitúa en mitad pero bastante aislado del resto de muestras. Lo mismo ocurre entre `drat` y `qsec`. Por esta razón es por la que quizás se esté catalogando como valor extremo. A continuación dibujamos un **BiPlot** con todas las variables marcando en rojo cuáles son los valores outliers descubiertos hasta el momento.

```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.norm, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```
### 4.4 Métodos basados en Clustering

#### 4.4.1 Clustering usando k-means

```{r}
num.outliers = 5
num.clusters = 3
set.seed(2)
kmeans.resultados = kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans = kmeans.resultados$cluster
head(asignaciones.clustering.kmeans)
centroides.normalizados = kmeans.resultados$centers
centroides.normalizados
# Centroides sin normalizar
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados
```
Vamos a implementar una nueva función para calcular los *outliers* a partir de las distancias de todos los puntos a los centroides de sus clústeres. Como se puede observar en el `biplot` y en el diagrama de cajas, muchos puntos han sido categorizados como *outliers* por la acumulación de valores más o menos extremos en varias columnas, pero realmente no tienen por qué serlo.

```{r}
top_clustering_outliers = function(datos.normalizados, asignaciones.clustering,  datos.centroides.normalizados, num.outliers) {
  distancias = distancias_a_centroides(datos.normalizados, asignaciones.clustering, datos.centroides.normalizados)
  outliers = sort(distancias, decreasing=TRUE)[1:num.outliers]
  claves = as.numeric(sapply(c(1:num.outliers), function(x) which(distancias==outliers[x])))
  return(list(claves=claves, distancias=outliers))
}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm, asignaciones.clustering.kmeans, centroides.normalizados, num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
claves.outliers.kmeans
nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
nombres.outliers.kmeans
distancias.outliers.centroides = top.outliers.kmeans$distancias
distancias.outliers.centroides
# Mostramos un biplot con la información de los outliers y de los clusters
biplot_outliers_clustering(datos.num, titulo = "Outliers k-means", asignaciones.clustering = asignaciones.clustering.kmeans, claves.outliers = claves.outliers.kmeans)
# Diagrama de cajas
diag_caja_juntos(datos.num, "Outliers k-means", claves.outliers.kmeans)
```

#### 4.4.2 Clustering usando medoides

```{r}
# Matriz de distancia todosxtodos
set.seed(2)
matriz.distancias = dist(datos.num.norm)
modelo.pam = pam(matriz.distancias , k = num.clusters)
# Mostramos las asignaciones de los clústeres y los medoides
asignaciones.clustering.pam = modelo.pam$clustering   
nombres.medoides = modelo.pam$medoids   
nombres.medoides
medoides = datos.num[nombres.medoides, ]
medoides.normalizados = datos.num.norm[nombres.medoides, ]
medoides.normalizados
# Calculamos los X primero outliers
top.outliers.kmedoides = top_clustering_outliers(datos.num.norm, asignaciones.clustering.pam, medoides.normalizados, num.outliers)
claves.outliers.pam = top.outliers.kmedoides$claves
claves.outliers.pam
nombres.outliers.pam = top.outliers.kmedoides$distancias
nombres.outliers.pam
# Biplot
biplot_outliers_clustering(datos.num, titulo = "Outliers k-medoids", asignaciones.clustering = asignaciones.clustering.pam, claves.outliers = claves.outliers.pam)
```

### 4.5 Análisis de los outliers multivariantes puros

Calculamos los outliers multivariantes puros a partir de los detectados con el método `LOF` y con `IQR`.

```{r}
# Calculamos los outliers multivariantes puros
claves.outliers.IQR.en.alguna.columna
claves.outliers.lof
claves.outliers.lof.no.IQR = setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
claves.outliers.lof.no.IQR
nombres_filas(datos.num.norm, claves.outliers.lof.no.IQR)
# Aumentamos el número de outliers 
num.outliers = 11
claves.outliers.IQR.en.alguna.columna
claves.outliers.lof = sapply(c(1:num.outliers), function(x) which(lof.scores==sorted.lof.scores[1:num.outliers][x]))
claves.outliers.lof
claves.outliers.lof.no.IQR = setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
claves.outliers.lof.no.IQR
nombres_filas(datos.num.norm, claves.outliers.lof.no.IQR)
# Biplot
result = biplot_2_colores(datos.num.norm, claves.outliers.lof.no.IQR)
result
# Datos normalizados de los outliers
datos.num.norm[claves.outliers.lof.no.IQR, ]
```


